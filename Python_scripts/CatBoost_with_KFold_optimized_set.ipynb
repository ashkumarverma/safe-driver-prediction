{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import *\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.37518191338\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train = pd.read_csv(\"/home/saurabhg/PuertoSergo/train.csv\")\n",
    "test = pd.read_csv(\"/home/saurabhg/PuertoSergo/test.csv\")\n",
    "print(time.time()-start)\n",
    " \n",
    "y = train['target']\n",
    "testid= test['id'].values\n",
    "\n",
    "train.drop(['id','target'],axis=1,inplace=True)\n",
    "test.drop(['id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Init Shape: ', (595212, 39))\n",
      "('After Shape: ', (595212, 136))\n",
      "('Init Shape: ', (892816, 39))\n",
      "('After Shape: ', (892816, 136))\n"
     ]
    }
   ],
   "source": [
    "### Drop calc\n",
    "unwanted = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "train = train.drop(unwanted, axis=1)  \n",
    "test = test.drop(unwanted, axis=1)\n",
    "\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "#train['ps_reg_03_square'] = train['ps_reg_03']*train['ps_reg_03']\n",
    "#test['ps_reg_03_square'] = test['ps_reg_03']*test['ps_reg_03']\n",
    "#train['ps_car_14_square'] = train['ps_car_14']*train['ps_car_14']\n",
    "#test['ps_car_14_square'] = test['ps_car_14']*test['ps_car_14']\n",
    "def recon(reg):\n",
    "    integer = int(np.round((40*reg)**2)) \n",
    "    for a in range(32):\n",
    "        if (integer - a) % 31 == 0:\n",
    "            A = a\n",
    "    M = (integer - A)//31\n",
    "    return A, M\n",
    "train['ps_reg_A'] = train['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "train['ps_reg_M'] = train['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "train['ps_reg_A'].replace(19,-1, inplace=True)\n",
    "train['ps_reg_M'].replace(51,-1, inplace=True)\n",
    "test['ps_reg_A'] = test['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "test['ps_reg_M'] = test['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "test['ps_reg_A'].replace(19,-1, inplace=True)\n",
    "test['ps_reg_M'].replace(51,-1, inplace=True)\n",
    "\n",
    "\n",
    "### Froza's baseline\n",
    "### Froza's baseline\n",
    "### Froza's baseline\n",
    "### Froza's baseline\n",
    "\n",
    "d_median = train.median(axis=0)\n",
    "d_mean = train.mean(axis=0)\n",
    "d_skew = train.skew(axis=0)\n",
    "one_hot = {c: list(train[c].unique()) for c in train.columns if c not in ['id','target']}\n",
    "\n",
    "def transform_df(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    dcol = [c for c in df.columns if c not in ['id','target']]\n",
    "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n",
    "    for c in dcol:\n",
    "        if '_bin' not in c: #standard arithmetic\n",
    "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\n",
    "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\n",
    "\n",
    "    for c in one_hot:\n",
    "        if len(one_hot[c])>2 and len(one_hot[c]) < 8:\n",
    "            for val in one_hot[c]:\n",
    "                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n",
    "    return df\n",
    "\n",
    "def multi_transform(df):\n",
    "    print('Init Shape: ', df.shape)\n",
    "    p = Pool(cpu_count())\n",
    "    df = p.map(transform_df, np.array_split(df, cpu_count()))\n",
    "    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "    p.close(); p.join()\n",
    "    print('After Shape: ', df.shape)\n",
    "    return df\n",
    "\n",
    "train = multi_transform(train)\n",
    "test = multi_transform(test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute gini\n",
    "\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "\n",
    "@jit\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "def gini_lgb(act, preds):\n",
    "#    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(act, preds)\n",
    "    return 'gini', gini_score,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 5000\n",
    "OPTIMIZE_ROUNDS = True\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "f_cats = [f for f in train.columns if \"_cat\" in f]\n",
    "# Note: I set EARLY_STOPPING_ROUNDS high so that (when OPTIMIZE_ROUNDS is set)\n",
    "#       I will get lots of information to make my own judgment.  You should probably\n",
    "#       reduce EARLY_STOPPING_ROUNDS if you want to do actual early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "n_estimators = 1000\n",
    "folds = KFold(n_splits=n_splits, shuffle=True, random_state=1) \n",
    "imp_df = np.zeros((len(train.columns), n_splits))\n",
    "oof = np.empty(len(train))\n",
    "sub_preds = np.zeros((len(test),n_splits))\n",
    "increase = False\n",
    "np.random.seed(0)\n",
    "params = {'eta': 0.025, 'max_depth': 4, \n",
    "          'subsample': 0.9, 'colsample_bytree': 0.7, \n",
    "          'colsample_bylevel':0.7,\n",
    "          'min_child_weight':100,\n",
    "          'alpha':10,\n",
    "          'objective': 'binary:logistic', 'eval_metric': 'auc', 'seed': 99, 'silent': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 : 0.281761 @1000 \n",
      "Fold  2 : 0.277750 @1000 \n",
      "Fold  3 : 0.275094 @1000 \n",
      "Fold  4 : 0.296755 @1000 \n",
      "Fold  5 : 0.282124 @1000 \n",
      "Full OOF score : 0.282477\n"
     ]
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(y, y)):\n",
    "    trn_dat, trn_tgt = train.iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_dat, val_tgt = train.iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    clf = CatBoostClassifier(iterations=n_estimators, \n",
    "                                      learning_rate=0.05,\n",
    "                                      depth=5,\n",
    "                                      rsm = 0.7,\n",
    "                                      loss_function='Logloss',\n",
    "                                      eval_metric=\"AUC\",\n",
    "                                      use_best_model = True,\n",
    "                                      random_strength = 20,\n",
    "                                #      calc_feature_importance=True,\n",
    "                                #      one_hot_max_size = 7, \n",
    "                                      l2_leaf_reg = 4,\n",
    "                                      random_seed=99,\n",
    "                                      od_type='Iter', \n",
    "                                      od_wait=70\n",
    "                                     )\n",
    "    \n",
    "    # Upsample during cross validation to avoid having the same samples\n",
    "    # in both train and validation sets\n",
    "    # Validation set is not up-sampled to monitor overfitting\n",
    "    if increase:\n",
    "        # Get positive examples\n",
    "        pos = pd.Series(trn_tgt == 1)\n",
    "        # Add positive examples\n",
    "        trn_dat = pd.concat([trn_dat, trn_dat.loc[pos]], axis=0)\n",
    "        trn_tgt = pd.concat([trn_tgt, trn_tgt.loc[pos]], axis=0)\n",
    "        # Shuffle data\n",
    "        idx = np.arange(len(trn_dat))\n",
    "        np.random.shuffle(idx)\n",
    "        trn_dat = trn_dat.iloc[idx]\n",
    "        trn_tgt = trn_tgt.iloc[idx]\n",
    "        \n",
    "    clf.fit(trn_dat, trn_tgt, \n",
    "            eval_set= (val_dat, val_tgt),\n",
    "      #      cat_features =indice,\n",
    "      #      eval_metric=gini_lgb,\n",
    "            use_best_model=True,\n",
    "            verbose=False)\n",
    "    # Find best round for validation setA\n",
    "    #lgb_evals[:, fold_] = clf.evals_result_[\"valid_1\"][\"gini\"]\n",
    "    #print(clf.evals_result_ )\n",
    "    # Keep feature importances\n",
    "    #imp_df[:, fold_] = clf.feature_importance_\n",
    "    #Xgboost provides best round starting from 0 so it has to be incremented\n",
    "    #best_round = np.argsort(lgb_evals[:, fold_])[::-1][0]\n",
    "\n",
    "    # Predict OOF and submission probas with the best round\n",
    "    oof[val_idx] = clf.predict_proba(val_dat)[:,1]\n",
    "    # Update submission\n",
    "    sub_preds[:, fold_] = clf.predict_proba(test)[:,1]\n",
    "\n",
    "    # Display results\n",
    "    print(\"Fold %2d : %.6f @%4d \"\n",
    "          % (fold_ + 1,\n",
    "             gini_normalized(val_tgt, oof[val_idx]),\n",
    "             n_estimators))\n",
    "          \n",
    "print(\"Full OOF score : %.6f\" % gini_normalized(y, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full OOF score : -0.001052\n"
     ]
    },
    {
     "ename": "CatboostError",
     "evalue": "There is no trained model to use predict(). Use fit() to train model. Then use predict().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatboostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-87b91ca20a8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Full OOF score : %.6f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgini_normalized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moof\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Update submission\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msub_preds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/puneetj/anaconda2/lib/python2.7/site-packages/catboost/core.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, data, weight, ntree_limit, verbose)\u001b[0m\n\u001b[0;32m   1026\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \"\"\"\n\u001b[1;32m-> 1028\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Probability'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Class'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/puneetj/anaconda2/lib/python2.7/site-packages/catboost/core.pyc\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, data, weight, prediction_type, ntree_limit, verbose)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mCatboostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"There is no trained model to use predict(). Use fit() to train model. Then use predict().\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_cat_feature_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCatboostError\u001b[0m: There is no trained model to use predict(). Use fit() to train model. Then use predict()."
     ]
    }
   ],
   "source": [
    "print(\"Full OOF score : %.6f\" % gini_normalized(y, oof))\n",
    "oof[val_idx] = clf.predict_proba(val_dat)[:,1]\n",
    "    # Update submission\n",
    "sub_preds[:, fold_] = clf.predict_proba(test)[:,1]\n",
    "\n",
    "    # Display results\n",
    "print(\"Fold %2d : %.6f @%4d \"\n",
    "          % (fold_ + 1,\n",
    "             gini_normalized(val_tgt, oof[val_idx]),\n",
    "             n_estimators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = testid\n",
    "sub[\"target\"] =scipy.stats.hmean(sub_preds,axis=1)\n",
    "sub.to_csv(\"submission_10312017_LGB_.286264_5kfold.csv\", index=False, float_format=\"%.9f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
