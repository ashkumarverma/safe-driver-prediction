{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import *\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.25394201279\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train = pd.read_csv(\"/home/saurabhg/PuertoSergo/train.csv\")\n",
    "test = pd.read_csv(\"/home/saurabhg/PuertoSergo/test.csv\")\n",
    "print(time.time()-start)\n",
    " \n",
    "y = train['target']\n",
    "testid= test['id'].values\n",
    "\n",
    "train.drop(['id','target'],axis=1,inplace=True)\n",
    "test.drop(['id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Init Shape: ', (595212, 39))\n",
      "('After Shape: ', (595212, 97))\n",
      "('Init Shape: ', (892816, 39))\n",
      "('After Shape: ', (892816, 97))\n"
     ]
    }
   ],
   "source": [
    "### Drop calc\n",
    "unwanted = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "train = train.drop(unwanted, axis=1)  \n",
    "test = test.drop(unwanted, axis=1)\n",
    "\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "### Great Recovery from Pascal's materpiece\n",
    "#train['ps_reg_03_square'] = train['ps_reg_03']*train['ps_reg_03']\n",
    "#test['ps_reg_03_square'] = test['ps_reg_03']*test['ps_reg_03']\n",
    "#train['ps_car_14_square'] = train['ps_car_14']*train['ps_car_14']\n",
    "#test['ps_car_14_square'] = test['ps_car_14']*test['ps_car_14']\n",
    "def recon(reg):\n",
    "    integer = int(np.round((40*reg)**2)) \n",
    "    for a in range(32):\n",
    "        if (integer - a) % 31 == 0:\n",
    "            A = a\n",
    "    M = (integer - A)//31\n",
    "    return A, M\n",
    "train['ps_reg_A'] = train['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "train['ps_reg_M'] = train['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "train['ps_reg_A'].replace(19,-1, inplace=True)\n",
    "train['ps_reg_M'].replace(51,-1, inplace=True)\n",
    "test['ps_reg_A'] = test['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "test['ps_reg_M'] = test['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "test['ps_reg_A'].replace(19,-1, inplace=True)\n",
    "test['ps_reg_M'].replace(51,-1, inplace=True)\n",
    "\n",
    "\n",
    "### Froza's baseline\n",
    "### Froza's baseline\n",
    "### Froza's baseline\n",
    "### Froza's baseline\n",
    "\n",
    "d_median = train.median(axis=0)\n",
    "d_mean = train.mean(axis=0)\n",
    "d_skew = train.skew(axis=0)\n",
    "one_hot = {c: list(train[c].unique()) for c in train.columns if c not in ['id','target']}\n",
    "\n",
    "def transform_df(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    dcol = [c for c in df.columns if c not in ['id','target']]\n",
    "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n",
    "    for c in dcol:\n",
    "        if '_bin' not in c: #standard arithmetic\n",
    "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\n",
    "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\n",
    "\n",
    "#    for c in one_hot:\n",
    "#        if len(one_hot[c])>2 and len(one_hot[c]) < 7:\n",
    "#            for val in one_hot[c]:\n",
    "#                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n",
    "    return df\n",
    "\n",
    "def multi_transform(df):\n",
    "    print('Init Shape: ', df.shape)\n",
    "    p = Pool(cpu_count())\n",
    "    df = p.map(transform_df, np.array_split(df, cpu_count()))\n",
    "    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "    p.close(); p.join()\n",
    "    print('After Shape: ', df.shape)\n",
    "    return df\n",
    "\n",
    "train = multi_transform(train)\n",
    "test = multi_transform(test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute gini\n",
    "\n",
    "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n",
    "\n",
    "@jit\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "def gini_lgb(act, preds):\n",
    "#    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(act, preds)\n",
    "    return 'gini', gini_score,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 5000\n",
    "OPTIMIZE_ROUNDS = True\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "f_cats = [f for f in train.columns if \"_cat\" in f]\n",
    "# Note: I set EARLY_STOPPING_ROUNDS high so that (when OPTIMIZE_ROUNDS is set)\n",
    "#       I will get lots of information to make my own judgment.  You should probably\n",
    "#       reduce EARLY_STOPPING_ROUNDS if you want to do actual early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "n_estimators = 1000\n",
    "folds = KFold(n_splits=n_splits, shuffle=True, random_state=1) \n",
    "imp_df = np.zeros((len(train.columns), n_splits))\n",
    "oof = np.empty(len(train))\n",
    "sub_preds = np.zeros((len(test),n_splits))\n",
    "increase = False\n",
    "np.random.seed(0)\n",
    "params = {'eta': 0.025, 'max_depth': 4, \n",
    "          'subsample': 0.9, 'colsample_bytree': 0.7, \n",
    "          'colsample_bylevel':0.7,\n",
    "          'min_child_weight':100,\n",
    "          'alpha':10,\n",
    "          'objective': 'binary:logistic', 'eval_metric': 'auc', 'seed': 99, 'silent': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\ttraining's gini: 0.258222\tvalid_1's gini: 0.245388\n",
      "[200]\ttraining's gini: 0.289817\tvalid_1's gini: 0.270081\n",
      "[300]\ttraining's gini: 0.308262\tvalid_1's gini: 0.280341\n",
      "[400]\ttraining's gini: 0.321174\tvalid_1's gini: 0.284069\n",
      "[500]\ttraining's gini: 0.331667\tvalid_1's gini: 0.285849\n",
      "[600]\ttraining's gini: 0.340508\tvalid_1's gini: 0.286166\n",
      "Early stopping, best iteration is:\n",
      "[646]\ttraining's gini: 0.344627\tvalid_1's gini: 0.286656\n",
      "Fold  1 : 0.286614 @1000 / best score is 696.000000 @ 645\n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\ttraining's gini: 0.260245\tvalid_1's gini: 0.249295\n",
      "[200]\ttraining's gini: 0.291649\tvalid_1's gini: 0.268771\n",
      "[300]\ttraining's gini: 0.309422\tvalid_1's gini: 0.275683\n",
      "[400]\ttraining's gini: 0.322553\tvalid_1's gini: 0.279349\n",
      "[500]\ttraining's gini: 0.332814\tvalid_1's gini: 0.281009\n",
      "[600]\ttraining's gini: 0.342011\tvalid_1's gini: 0.281599\n",
      "Early stopping, best iteration is:\n",
      "[558]\ttraining's gini: 0.338214\tvalid_1's gini: 0.281762\n",
      "Fold  2 : 0.281729 @1000 / best score is 608.000000 @ 557\n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\ttraining's gini: 0.262331\tvalid_1's gini: 0.238006\n",
      "[200]\ttraining's gini: 0.293752\tvalid_1's gini: 0.259487\n",
      "[300]\ttraining's gini: 0.312746\tvalid_1's gini: 0.269224\n",
      "[400]\ttraining's gini: 0.325793\tvalid_1's gini: 0.272583\n",
      "[500]\ttraining's gini: 0.335987\tvalid_1's gini: 0.273713\n",
      "[600]\ttraining's gini: 0.345075\tvalid_1's gini: 0.274188\n",
      "[700]\ttraining's gini: 0.353234\tvalid_1's gini: 0.274736\n",
      "Early stopping, best iteration is:\n",
      "[680]\ttraining's gini: 0.351688\tvalid_1's gini: 0.274875\n",
      "Fold  3 : 0.274866 @1000 / best score is 730.000000 @ 679\n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\ttraining's gini: 0.256166\tvalid_1's gini: 0.26412\n",
      "[200]\ttraining's gini: 0.287222\tvalid_1's gini: 0.284711\n",
      "[300]\ttraining's gini: 0.306141\tvalid_1's gini: 0.293312\n",
      "[400]\ttraining's gini: 0.319365\tvalid_1's gini: 0.296202\n",
      "[500]\ttraining's gini: 0.329892\tvalid_1's gini: 0.297988\n",
      "[600]\ttraining's gini: 0.33887\tvalid_1's gini: 0.298691\n",
      "[700]\ttraining's gini: 0.347239\tvalid_1's gini: 0.299402\n",
      "Early stopping, best iteration is:\n",
      "[746]\ttraining's gini: 0.351208\tvalid_1's gini: 0.29978\n",
      "Fold  4 : 0.299739 @1000 / best score is 796.000000 @ 745\n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\ttraining's gini: 0.262111\tvalid_1's gini: 0.24415\n",
      "[200]\ttraining's gini: 0.291014\tvalid_1's gini: 0.265699\n",
      "[300]\ttraining's gini: 0.309261\tvalid_1's gini: 0.276666\n",
      "[400]\ttraining's gini: 0.32215\tvalid_1's gini: 0.282244\n",
      "[500]\ttraining's gini: 0.3324\tvalid_1's gini: 0.284821\n",
      "[600]\ttraining's gini: 0.341562\tvalid_1's gini: 0.286673\n",
      "[700]\ttraining's gini: 0.350095\tvalid_1's gini: 0.287459\n",
      "[800]\ttraining's gini: 0.357948\tvalid_1's gini: 0.287461\n",
      "Early stopping, best iteration is:\n",
      "[752]\ttraining's gini: 0.354221\tvalid_1's gini: 0.287642\n",
      "Fold  5 : 0.287630 @1000 / best score is 802.000000 @ 751\n",
      "Full OOF score : 0.285902\n"
     ]
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(y, y)):\n",
    "    trn_dat, trn_tgt = train.iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_dat, val_tgt = train.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    clf = lgb.LGBMModel(n_estimators=n_estimators,\n",
    "                        num_leaves=16,\n",
    "   #                     boosting_type = 'dart',\n",
    "                        objective=\"binary\",\n",
    "                        learning_rate=.025, \n",
    "                        subsample=.9, \n",
    "                        colsample_bytree=.7,\n",
    "         #               min_split_gain = .77,\n",
    "        #                min_child_samples= 500,\n",
    "         #               is_unbalance = True,\n",
    "                        min_child_weight =100,\n",
    "                        reg_alpha=4,\n",
    "          #              reg_lambda=2,\n",
    "                      #  nthread=2\n",
    "                       )\n",
    "    # Upsample during cross validation to avoid having the same samples\n",
    "    # in both train and validation sets\n",
    "    # Validation set is not up-sampled to monitor overfitting\n",
    "    if increase:\n",
    "        # Get positive examples\n",
    "        pos = pd.Series(trn_tgt == 1)\n",
    "        # Add positive examples\n",
    "        trn_dat = pd.concat([trn_dat, trn_dat.loc[pos]], axis=0)\n",
    "        trn_tgt = pd.concat([trn_tgt, trn_tgt.loc[pos]], axis=0)\n",
    "        # Shuffle data\n",
    "        idx = np.arange(len(trn_dat))\n",
    "        np.random.shuffle(idx)\n",
    "        trn_dat = trn_dat.iloc[idx]\n",
    "        trn_tgt = trn_tgt.iloc[idx]\n",
    "        \n",
    "    clf.fit(trn_dat, trn_tgt, \n",
    "            eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n",
    "            categorical_feature = f_cats,\n",
    "            eval_metric=gini_lgb,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100)\n",
    "    # Find best round for validation setA\n",
    "    lgb_evals = clf.evals_result_[\"valid_1\"][\"gini\"]\n",
    "    #print(clf.evals_result_ )\n",
    "    # Keep feature importances\n",
    "    imp_df[:, fold_] = clf.feature_importances_\n",
    "    #Xgboost provides best round starting from 0 so it has to be incremented\n",
    "    best_round = np.argsort(lgb_evals)[::-1][0]\n",
    "\n",
    "    # Predict OOF and submission probas with the best round\n",
    "    oof[val_idx] = clf.predict(val_dat, num_iteration=best_round)\n",
    "    # Update submission\n",
    "    sub_preds[:, fold_] = clf.predict(test, num_iteration=best_round)\n",
    "\n",
    "    # Display results\n",
    "    print(\"Fold %2d : %.6f @%4d / best score is %.6f @%4d\"\n",
    "          % (fold_ + 1,\n",
    "             gini_normalized(val_tgt, oof[val_idx]),\n",
    "             n_estimators,\n",
    "             len(lgb_evals),\n",
    "             best_round))\n",
    "          \n",
    "print(\"Full OOF score : %.6f\" % gini_normalized(y, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_pred= np.delete(sub_preds,2,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = testid\n",
    "sub[\"target\"] =scipy.stats.hmean(sub_pred,axis=1)\n",
    "sub.to_csv(\"submission_10312017_LGB_.0.285902_5kfold_4pred.csv\", index=False, float_format=\"%.9f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
